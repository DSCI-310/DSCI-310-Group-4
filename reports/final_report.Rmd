---
title: 'DSCI 310 Group 4: Default payments in Taiwan and comparison of the predictive accuracy of probability of default'
author: "Shravan Chaniara, Jordan Yu, Diana Liang, Hannah Martin"
output:
  bookdown::html_document2:
    toc: true
    number_sections: false
  bookdown::pdf_document2:
    toc: true
    number_sections: false
    
bibliography: references.bib
---
# Abstract

Financial institutions incur monetary loss when a client or borrower is unable to pay their interest or their initial principal on time. Thus, it is necessary for such institutions to assess the risk that potential borrowers cannot repay their loan in determining their eligibility for the loan in the first place. The present study endeavors to answer the question "Is there a way to effectively predict whether or not a client will default on their credit card payment?" and uncover the most significant features that contribute to the higher likelihood of defaulting. The result of predictive accuracy of the projected likelihood of default will be more beneficial than the binary result of categorization - credible or not credible customers - from the standpoint of risk management.

# Introduction

Amidst growing financial insecurity during the pandemic, unsecured debt has continued to rise (@Frech2021). Consequently, the consumer credit market and risk prediction has been a matter of great speculation and fear, lest there be a repeat of the financial crises that rocked the economic world in the late 2000s: In 2006, Taiwan was rocked by a credit card debt crisis with debt from credit cards and cash cards reaching $268 billion USD and over half a million people unable to repay their loans (@Yeh2009). As many could barely afford to pay the minimum credit card debt balance every month or continued to default on their payments, significant societal problems consequently plagued the country, many banks incurred heavy losses and the government eventually needed to step in to stabilize the financial system (@Yeh2009). This situation arose because many banks in Taiwan had lowered the requirements for credit card approval in order to gain more customers within the increasingly competitive industry (@Tsai2010). Such examples indicate that a strict assessment of an applicant’s capability to make their card payment is critical to a well-developed financial system and a business’s survivability in the banking industry.

This project focuses on the case of customers default payments in Taiwan and finds the predictive accuracy of the probability of the customers to default. The purpose of this study is to assess the true probability of default because the real probability of default is unknown.

## Dataset Information

This project used the data @default_credit_card from UCI Machine learning repository. As the response variable, this project used data from a binary variable, `default payment` (Yes = 1, No = 0). The following 23 factors were considered as explanatory variables in this study, which was based on a review of the literature:

* Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.
* Gender (1 = male; 2 = female).
* Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).
* Marital status (1 = married; 2 = single; 3 = others).
* Age (year).
* History of past payment. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months . . . 8 = payment delay for eight months; 9 = payment delay for nine months and above.
* Amount of bill statement (NT dollar).
* Amount of previous payment (NT dollar).

The objective of this project is to maintain ease of interpretation for the average reader. In line with this goal, we will simplify the models and methods of analysis we choose to use as well as exclude some features in the data set in favor of greater readability.

# Methods and Results

### Exploratory Data Analysis

First, we load and tidy the data. The dataset was split into a 80% training and 20% testing set. The model will be built using only the training data. This gives the abilithy to compute a final performance metric for our model by evaluating it on the testing data. train_test_split() function shuffles the data to ensure the data ending up in the training and test sets is randomized.

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
#library(tidyverse)
#library(tidymodels)
```

Before any data analysis can be done let's check if the dataset has null/missing values that may affect further analysis: The dataset is super clean, and no missing value is found.

### Statistics information

This part we have a look at some basic statistics of the training set:

````{r attribute-box, echo=FALSE, fig.cap="Summary Stats.", out.width = "80%", warning = FALSE, message = FALSE}
summary_stat <- read.csv("../results/summary_stats.csv")
knitr::kable(summary_stat, caption = "Summary Stats")
````

We can see the following information:

* The average amount of given credit in NT dollars is 166849.320000;
* The average age of all clients is 35.419625, etc.

To better understand the correlation between variables, we would like to compute and visualize the correlations:

```{r heat-map, echo=FALSE, fig.cap="Heat Map", out.width = "80%", fig.align="center"}
knitr::include_graphics("../results/heatmap.png")
```

The heatmap shows some positive/negative correlations:

Positive correlations:

* `Default payment` - `PAY_0` to `PAY_6` (Repayment status from April to September, 2005);
* `Limit balance` - `BILL_AMT1` to `BILL_AMT6` (Amount of bill statement from April to September, 2005), etc.

Negative correlations:

* `Limit balance` - `PAY_0` to `PAY_6` (Repayment status from April to September, 2005), etc.

Specifically, `PAY_0` has the highest correlation with `default_payment`. This will give us a signal that `PAY_0` plays an important role for predicting `default_payment`.

## Exploring variables

### LIMIT_BAL

First, we look at the amount of given credit (in NT dollars). Credit card limits are likely an indictor of how wealthy someone is since banks tend to give higher limits to clients that have more money with them. Thus, this may be an important feature when predicting if someone is able to pay the bill on time.

```{r limit-dist, echo=FALSE, out.width = "80%", fig.cap="Limit Balance Distribution"}
knitr::include_graphics("../results/limit_bal_dist.png")
```

### Repayment Status

```{r repayment-status-PAY-0, echo=FALSE, out.width = "80%", fig.align="center", fig.cap="Repayment Status PAY_0"}
knitr::include_graphics("../results/repayment_status_PAY_0.png")
```

```{r repayment-status-PAY-2, echo=FALSE, out.width = "80%", fig.align="center", fig.cap="Repayment Status PAY_2"}
knitr::include_graphics("../results/repayment_status_PAY_2.png")
```

```{r repayment-status-PAY-3, echo=FALSE, out.width = "80%",fig.align="center", fig.cap="Repayment Status PAY_3"}
knitr::include_graphics("../results/repayment_status_PAY_3.png")
```

```{r repayment-status-PAY-4, echo=FALSE, out.width = "80%",fig.align="center", fig.cap="Repayment Status PAY_4"}
knitr::include_graphics("../results/repayment_status_PAY_4.png")
```

```{r repayment-status-PAY-5, echo=FALSE, out.width = "80%",fig.align="center", fig.cap="Repayment Status PAY_5"}
knitr::include_graphics("../results/repayment_status_PAY_5.png")
```

```{r repayment-status-PAY-6, echo=FALSE, out.width = "80%",fig.align="center", fig.cap="Repayment Status PAY_6"}
knitr::include_graphics("../results/repayment_status_PAY_6.png")
```

Looking at the above plots on repayment status shows that if a client defaults on their payment for 2 months (e.g PAY_X = 2), it is a indicator to predict that default_payment = 1. It looks like repayment status will be an important feature in the model.

## Analysis

### Class Imbalance

```{r class-imbalance, echo=FALSE, out.width = "80%", fig.align="center", fig.cap="Count Plot"}
knitr::include_graphics("../results/class_imbalance.png")
```
The above plot shows the percentage of rows with default payment = 0 versus default payment = 1. Cleary there is class imbalance in this dataset. Because of this, we will use the area under the Receiver Operating Characterisitc curve (ROC AUC) as our primary metric to evaluate our model instead of accuracy, which tends to be missleading in cases of class imbalance. ROC AUC evalauates how good the model is as distinguishing between classes, and gives a more accurate sense of how well our model generalizes when dealing with class imbalance.

### Preprossesing

We apply scaling to numeric features to ensure the model built will be robust and not sensitive to the scale of each individual feature. To do this, we use the StandardScaler() function to set the sample mean to 0 and standard deviation to 1.

To handle categorical values, we will apply one-hot-encoding. This creates binary dummy variables for each category. Next, we apply scaling and one-hot-encoding through using a column transformer, which applies the transformations to each column specified.

### Model 0: Dummy Classifier

Firstly, we will try a baseline model to act as a comparison measure for the final model built. Using a pipeline to do this ensures that the model is built using just the training data, and that the testing data has no influence on the model. 5-fold Cross validation is used to give a more robust measure of performance error. K-fold Cross-validation spilts the training data into k folds, and each time one fold is the validation set. Each fold fits the model on the training portion and uses one fold as a validation set to calculate a performance metric, which can be averaged to get a overall score of how well the model does. This ensures that outliers don't negatively influence the performance metric.

````{r score-1, echo=FALSE, warning = FALSE, message = FALSE}
score_1 <- read.csv("../results/scores_1.csv")
knitr::kable(score_1)
````

### Model 1: Logistic Regression

Next, a logistic regression model is fitted to the training data. Logistic regression uses the training data to learn coefficients, which then can be used to calculate prediction probabilities of the each class using the sigmoid function. This allows us to calculate the probability of a client defaulting on their credit card based on their data. The hyperparameter C is used to control the fundatamental tradeoff of bias and variance, to reduce the likeliness of the model overfitting or underfitting. We chose logistic regression over more complex classifiers such as ensemble trees because it is easier to interpret and efficient to train. The predicted coeficients give information about feature importance and direction of association, making the model easily interpreted. Moreover, since the dataset is significantly larger than the number of features, logistic regression is less likely to overfit because it is a low variance model.

Hyperparameter optimization is carried out to find the best value of C for the data in hopes to reduce bias and variance. Cross-validation is used to test how well the model performs on unseen data during hyperparemter opimization, enabling performance metrics to be calculated to compare different values of C.

````{r score-2, echo=FALSE, warning = FALSE, message = FALSE}
score_2 <- read.csv("../results/scores_2.csv")
knitr::kable(score_2)
````

Next, the logistic regression model with the optimized C value is fitted to the training data. The model is built using pipelines to ensure all data preprocessing is constant and that no information from the testing set leaks into the training of the model.

### Feature Importance

Next, we will look at feature importance. In logistic regression, the magnitude and direction of the learned coefficients explain the relationship between a explanatory feature and the response variable.

````{r coef, echo=FALSE, warning = FALSE, message = FALSE}
coefs <- read.csv("../results/coefs.csv")
knitr::kable(coefs)
````

The table above shows the most important features according to the model. Positive coefficients indicate that an increase in the feature increases the probability that the response variable is class 1, and negative coefficients indicate that a increases in the feature decreases the probability that the response variable is class 1.

The most important feature is repayment status in April with a 2 month delay.

To further interpret this, the odds ratio can be calculated. $OR = e^{\beta}$,  where $\beta$ is a model coefficient.

The odds ratio for x0_2 is 2.8901590141210964 can be interpreted as clients who delayed payment for 2 months in April have about 2.9 times the odds of defaulting their credit card payment next month than those who did not delay for 2 months of, controlling for the other features since there is some correlation.

It makes logical sense that this feature is important since it was highly correlated with the response variable during the exploratory data analysis, and because it makes sense that if someone has delayed payment in the past they may not be able to pay in the future as well.

Another important feature is x0_0, corresponding to repayment status in April with a revolving credit. The magnitude is negative here meaning that a value of 1 for this binary variable is negatively associated with defaulting next month. This makes sense because revolving credit lets clients pay a minimum balance instead of the full bill, making them less likely to default. The odds ratio is calculated below.


The odds ratio is 0.3662019170982376 shows that clients who had a repayment status of using a revolving credit in April decreases the odds of class 1 versus class 0 by about 63%, controlling for the other features.

Due to the large number of features, we only looked at the first 2 most important features, however this method can be done for the remaining features as well.

### Testing the Model

Finally, the model will be evaluated by predicting on the test dataset. Performance metrics including area under the ROC curve, f1-score, precision and recall will be calculated. Precision is the ratio of true postive and total positives. Recall is the measure of the model correctly identifying true positives. F1-score is the harmonic mean of precision and recall.

```{r roc, echo=FALSE, out.width = "80%", fig.align="center", fig.cap="ROC"}
knitr::include_graphics("../results/roc.png")
```

The test ROC AUC is almost the same as the cross-validation ROC AUC so we can conclude that there is little optimization bias and we are not overfitting. Area under the ROC curve is the classifier's ability to distinguish between classes. So 78% of the time, the model is able to correctly distinguish between class 0 and class 1.

````{r metrics, echo=FALSE, warning = FALSE, message = FALSE}
metrics <- read.csv("../results/metrics.csv")
knitr::kable(metrics)
````
```{r confusion_matrix, echo=FALSE, out.width = "80%", fig.align="center", fig.cap="Confusion matrix"}
knitr::include_graphics("../results/confusion_matrix.png")
```

It appears that the model does significantly better for the non-default class, since precision, recall, and f1-score are very high. Recall is partically low for the default class, and very high for the non-default class. Thus this model is very good at indentifying clients who will pay their bill on time. Precision can be intrepreted as when the model predicts class 0 (non-default) it is correct 84% of the time, and when it predicts class 1 it is correct 64% of the time, which is not as good.


# Discussion

Looking at the results above, the analysis has a few limitations. One of the main limitations is the extrapolation of data. The dataset used was from Taiwan so the model may not apply properly for financial institutions and people outside of Taiwan. The other limitation is that logistic regression favors interpretability over prediction accuracy. The coefficients are easy to understand while the logistic regression is not as accurate as other algorithms.

Another limitation of the analysis is that logistic regression is a higher bias model than other classifiers such as ensemble trees. This means that the model pays less attention to the training data and may oversimplify it, resulting in less accuracy. However, the results show that the model did very well when predicting clients who did not default next month (class 0). The high recall and precision score for class 0 suggest this model can be very useful when trying to distinguish if a client will pay the bill on time. Thus, this model can be used to help financial institutions make various risk decisions about a client including whether to offer someone a loan or whether to raise the credit card monthly limit. A prediction probability can be calculated based on the input features of each individual client. A prediction probability close to 0 indicates that the client is likely to pay the credit card bill.

According to the model, the most important feature was repayment status. Specifically, whether or not someone delayed payment for 2 months, and whether or not someone used a revolving credit card. This provides insight into what features financial institutions should look for when assessing risk for individual clients. Also, the model suggests that giving someone a revolving credit card where they only have to pay a minimum balance instead of the whole bill each month reduces the probability of the client defaulting. This information could be used to prevent the institute from losing money due to people defaulting.

Overall, the model provides insight into important features that may benefit financial institutions when managing risk, and allows an institute to calculate the probability that an individual client will default.

# References